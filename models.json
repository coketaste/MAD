[
  {
    "name": "pyt_huggingface_gpt2",
    "url": "https://github.com/huggingface/transformers",
    "dockerfile": "docker/pyt_huggingface",
    "scripts": "scripts/huggingface_gpt2/run.sh",
    "n_gpus": "-1",
    "owner": "mad.support@amd.com",
    "training_precision": "fp16",
    "tags": [
      "pyt",
      "fp16",
      "gpt2"
    ],
    "args": ""
  },
  {
    "name": "pyt_huggingface_bert",
    "url": "https://github.com/huggingface/transformers",
    "dockerfile": "docker/pyt_huggingface",
    "scripts": "scripts/huggingface_bert/run.sh",
    "n_gpus": "-1",
    "owner": "mad.support@amd.com",
    "training_precision": "",
    "tags": [
      "pyt",
      "bert"
    ],
    "args": ""
  },
  {
    "name": "pyt_vllm_llama-3.1-8b",
    "url": "",
    "dockerfile": "docker/pyt_vllm",
    "scripts": "scripts/vllm/run.sh",
    "data": "huggingface",
    "n_gpus": "-1",
    "owner": "mad.support@amd.com",
    "training_precision": "",
    "multiple_results": "perf_Llama-3.1-8B-Instruct.csv",
    "tags": [
      "pyt",
      "vllm"
    ],
    "timeout": -1,
    "args":
     "--model_repo meta-llama/Llama-3.1-8B-Instruct --test_option latency,throughput --num_gpu 1 --datatype float16 --tunableop off"
  },
  {
    "name": "pyt_vllm_llama-3.1-70b",
    "url": "",
    "dockerfile": "docker/pyt_vllm",
    "scripts": "scripts/vllm/run.sh",
    "data": "huggingface",
    "n_gpus": "-1",
    "owner": "mad.support@amd.com",
    "training_precision": "",
    "multiple_results": "perf_Llama-3.1-70B-Instruct.csv",
    "tags": [
      "pyt",
      "vllm"
    ],
    "timeout": -1,
    "args":
     "--model_repo meta-llama/Llama-3.1-70B-Instruct --test_option latency,throughput --num_gpu 8 --datatype float16 --tunableop off"
  },
  {
    "name": "pyt_vllm_llama-3.1-405b",
    "url": "",
    "dockerfile": "docker/pyt_vllm",
    "scripts": "scripts/vllm/run.sh",
    "data": "huggingface",
    "n_gpus": "-1",
    "owner": "mad.support@amd.com",
    "training_precision": "",
    "multiple_results": "perf_Llama-3.1-405B-Instruct.csv",
    "tags": [
      "pyt",
      "vllm"
    ],
    "timeout": -1,
    "args":
     "--model_repo meta-llama/Llama-3.1-405B-Instruct --test_option latency,throughput --num_gpu 8 --datatype float16 --tunableop off"
  },
  {
    "name": "pyt_vllm_llama-3.2-11b-vision-instruct",
    "url": "",
    "dockerfile": "docker/pyt_vllm",
    "scripts": "scripts/vllm/run.sh",
    "data": "huggingface",
    "n_gpus": "-1",
    "owner": "mad.support@amd.com",
    "training_precision": "",
    "multiple_results": "perf_Llama-3.2-11B-Vision-Instruct.csv",
    "tags": [
      "pyt",
      "vllm"
    ],
    "timeout": -1,
    "args":
     "--model_repo meta-llama/Llama-3.2-11B-Vision-Instruct --test_option latency,throughput --num_gpu 1 --datatype float16 --tunableop off"
  },
  {
    "name": "pyt_vllm_llama-2-7b",
    "url": "",
    "dockerfile": "docker/pyt_vllm",
    "scripts": "scripts/vllm/run.sh",
    "data": "huggingface",
    "n_gpus": "-1",
    "owner": "mad.support@amd.com",
    "training_precision": "",
    "multiple_results": "perf_Llama-2-7b-chat-hf.csv",
    "tags": [
      "pyt",
      "vllm"
    ],
    "timeout": -1,
    "args":
     "--model_repo meta-llama/Llama-2-7b-chat-hf --test_option latency,throughput --num_gpu 1 --datatype float16 --tunableop off"
  },
  {
    "name": "pyt_vllm_llama-2-70b",
    "url": "",
    "dockerfile": "docker/pyt_vllm",
    "scripts": "scripts/vllm/run.sh",
    "data": "huggingface",
    "n_gpus": "-1",
    "owner": "mad.support@amd.com",
    "training_precision": "",
    "multiple_results": "perf_Llama-2-70b-chat-hf.csv",
    "tags": [
      "pyt",
      "vllm"
    ],
    "timeout": -1,
    "args":
     "--model_repo meta-llama/Llama-2-70b-chat-hf --test_option latency,throughput --num_gpu 8 --datatype float16 --tunableop off"
  },
  {
    "name": "pyt_vllm_mixtral-8x7b",
    "url": "",
    "dockerfile": "docker/pyt_vllm",
    "scripts": "scripts/vllm/run.sh",
    "data": "huggingface",
    "n_gpus": "-1",
    "owner": "mad.support@amd.com",
    "training_precision": "",
    "multiple_results": "perf_Mixtral-8x7B-Instruct-v0.1.csv",
    "tags": [
      "pyt",
      "vllm"
    ],
    "timeout": -1,
    "args":
     "--model_repo mistralai/Mixtral-8x7B-Instruct-v0.1 --test_option latency,throughput --num_gpu 8 --datatype float16 --tunableop off"
  },
  {
    "name": "pyt_vllm_mixtral-8x22b",
    "url": "",
    "dockerfile": "docker/pyt_vllm",
    "scripts": "scripts/vllm/run.sh",
    "data": "huggingface",
    "n_gpus": "-1",
    "owner": "mad.support@amd.com",
    "training_precision": "",
    "multiple_results": "perf_Mixtral-8x22B-Instruct-v0.1.csv",
    "tags": [
      "pyt",
      "vllm"
    ],
    "timeout": -1,
    "args":
     "--model_repo mistralai/Mixtral-8x22B-Instruct-v0.1 --test_option latency,throughput --num_gpu 8 --datatype float16 --tunableop off"
  },
  {
    "name": "pyt_vllm_mistral-7b",
    "url": "",
    "dockerfile": "docker/pyt_vllm",
    "scripts": "scripts/vllm/run.sh",
    "data": "huggingface",
    "n_gpus": "-1",
    "owner": "mad.support@amd.com",
    "training_precision": "",
    "multiple_results": "perf_Mistral-7B-Instruct-v0.3.csv",
    "tags": [
      "pyt",
      "vllm"
    ],
    "timeout": -1,
    "args":
     "--model_repo mistralai/Mistral-7B-Instruct-v0.3 --test_option latency,throughput --num_gpu 1 --datatype float16 --tunableop off"
  },
  {
    "name": "pyt_vllm_qwen2-7b",
    "url": "",
    "dockerfile": "docker/pyt_vllm",
    "scripts": "scripts/vllm/run.sh",
    "data": "huggingface",
    "n_gpus": "-1",
    "owner": "mad.support@amd.com",
    "training_precision": "",
    "multiple_results": "perf_Qwen2-7B-Instruct.csv",
    "tags": [
      "pyt",
      "vllm"
    ],
    "timeout": -1,
    "args":
     "--model_repo Qwen/Qwen2-7B-Instruct --test_option latency,throughput --num_gpu 1 --datatype float16 --tunableop off"
  },
  {
    "name": "pyt_vllm_qwen2-72b",
    "url": "",
    "dockerfile": "docker/pyt_vllm",
    "scripts": "scripts/vllm/run.sh",
    "data": "huggingface",
    "n_gpus": "-1",
    "owner": "mad.support@amd.com",
    "training_precision": "",
    "multiple_results": "perf_Qwen2-72B-Instruct.csv",
    "tags": [
      "pyt",
      "vllm"
    ],
    "timeout": -1,
    "args":
     "--model_repo Qwen/Qwen2-72B-Instruct --test_option latency,throughput --num_gpu 8 --datatype float16 --tunableop off"
  },
  {
    "name": "pyt_vllm_jais-13b",
    "url": "",
    "dockerfile": "docker/pyt_vllm",
    "scripts": "scripts/vllm/run.sh",
    "data": "huggingface",
    "n_gpus": "-1",
    "owner": "mad.support@amd.com",
    "training_precision": "",
    "multiple_results": "perf_jais-13b-chat.csv",
    "tags": [
      "pyt",
      "vllm"
    ],
    "timeout": -1,
    "args":
     "--model_repo core42/jais-13b-chat --test_option latency,throughput --num_gpu 1 --datatype float16 --tunableop off"
  },
  {
    "name": "pyt_vllm_jais-30b",
    "url": "",
    "dockerfile": "docker/pyt_vllm",
    "scripts": "scripts/vllm/run.sh",
    "data": "huggingface",
    "n_gpus": "-1",
    "owner": "mad.support@amd.com",
    "training_precision": "",
    "multiple_results": "perf_jais-30b-chat-v3.csv",
    "tags": [
      "pyt",
      "vllm"
    ],
    "timeout": -1,
    "args":
     "--model_repo core42/jais-30b-chat-v3 --test_option latency,throughput --num_gpu 1 --datatype float16 --tunableop off"
  },
  {
    "name": "pyt_vllm_dbrx-instruct",
    "url": "",
    "dockerfile": "docker/pyt_vllm",
    "scripts": "scripts/vllm/run.sh",
    "data": "huggingface",
    "n_gpus": "-1",
    "owner": "mad.support@amd.com",
    "training_precision": "",
    "multiple_results": "perf_dbrx-instruct.csv",
    "tags": [
      "pyt",
      "vllm"
    ],
    "timeout": -1,
    "args":
     "--model_repo databricks/dbrx-instruct --test_option latency,throughput --num_gpu 8 --datatype float16 --tunableop off"
  },
  {
    "name": "pyt_vllm_gemma-2-27b",
    "url": "",
    "dockerfile": "docker/pyt_vllm",
    "scripts": "scripts/vllm/run.sh",
    "data": "huggingface",
    "n_gpus": "-1",
    "owner": "mad.support@amd.com",
    "training_precision": "",
    "multiple_results": "perf_gemma-2-27b.csv",
    "tags": [
      "pyt",
      "vllm"
    ],
    "timeout": -1,
    "args":
     "--model_repo google/gemma-2-27b --test_option latency,throughput --num_gpu 1 --datatype float16 --tunableop off"
  },
  {
    "name": "pyt_vllm_c4ai-command-r-plus-08-2024",
    "url": "",
    "dockerfile": "docker/pyt_vllm",
    "scripts": "scripts/vllm/run.sh",
    "data": "huggingface",
    "n_gpus": "-1",
    "owner": "mad.support@amd.com",
    "training_precision": "",
    "multiple_results": "perf_c4ai-command-r-plus-08-2024.csv",
    "tags": [
      "pyt",
      "vllm"
    ],
    "timeout": -1,
    "args":
     "--model_repo CohereForAI/c4ai-command-r-plus-08-2024 --test_option latency,throughput --num_gpu 8 --datatype float16 --tunableop off"
  },
  {
    "name": "pyt_vllm_deepseek-moe-16b-chat",
    "url": "",
    "dockerfile": "docker/pyt_vllm",
    "scripts": "scripts/vllm/run.sh",
    "data": "huggingface",
    "n_gpus": "-1",
    "owner": "mad.support@amd.com",
    "training_precision": "",
    "multiple_results": "perf_deepseek-moe-16b-chat.csv",
    "tags": [
      "pyt",
      "vllm"
    ],
    "timeout": -1,
    "args":
     "--model_repo deepseek-ai/deepseek-moe-16b-chat --test_option latency,throughput --num_gpu 1 --datatype float16 --tunableop off"
  },
  {
    "name": "pyt_vllm_llama-3.1-70b_fp8",
    "url": "",
    "dockerfile": "docker/pyt_vllm",
    "scripts": "scripts/vllm/run.sh",
    "data": "huggingface",
    "n_gpus": "-1",
    "owner": "mad.support@amd.com",
    "training_precision": "",
    "multiple_results": "perf_Llama-3.1-70B-Instruct-FP8-KV.csv",
    "tags": [
      "pyt",
      "vllm"
    ],
    "timeout": -1,
    "args":
     "--model_repo amd/Llama-3.1-70B-Instruct-FP8-KV --test_option latency,throughput --num_gpu 8 --datatype float8 --tunableop off"
  },
  {
    "name": "pyt_vllm_llama-3.1-405b_fp8",
    "url": "",
    "dockerfile": "docker/pyt_vllm",
    "scripts": "scripts/vllm/run.sh",
    "data": "huggingface",
    "n_gpus": "-1",
    "owner": "mad.support@amd.com",
    "training_precision": "",
    "multiple_results": "perf_Llama-3.1-405B-Instruct-FP8-KV.csv",
    "tags": [
      "pyt",
      "vllm"
    ],
    "timeout": -1,
    "args":
     "--model_repo amd/Llama-3.1-405B-Instruct-FP8-KV --test_option latency,throughput --num_gpu 8 --datatype float8 --tunableop off"
  },
  {
    "name": "pyt_vllm_mixtral-8x7b_fp8",
    "url": "",
    "dockerfile": "docker/pyt_vllm",
    "scripts": "scripts/vllm/run.sh",
    "data": "huggingface",
    "n_gpus": "-1",
    "owner": "mad.support@amd.com",
    "training_precision": "",
    "multiple_results": "perf_Mixtral-8x7B-Instruct-v0.1-FP8-KV.csv",
    "tags": [
      "pyt",
      "vllm"
    ],
    "timeout": -1,
    "args":
     "--model_repo amd/Mixtral-8x7B-Instruct-v0.1-FP8-KV --test_option latency,throughput --num_gpu 8 --datatype float8 --tunableop off"
  },
  {
    "name": "pyt_vllm_mixtral-8x22b_fp8",
    "url": "",
    "dockerfile": "docker/pyt_vllm",
    "scripts": "scripts/vllm/run.sh",
    "data": "huggingface",
    "n_gpus": "-1",
    "owner": "mad.support@amd.com",
    "training_precision": "",
    "multiple_results": "perf_Mixtral-8x22B-Instruct-v0.1-FP8-KV.csv",
    "tags": [
      "pyt",
      "vllm"
    ],
    "timeout": -1,
    "args":
    "--model_repo amd/Mixtral-8x22B-Instruct-v0.1-FP8-KV --test_option latency,throughput --num_gpu 8 --datatype float8 --tunableop off"
  },
  {
    "name": "pyt_vllm_mistral-7b_fp8",
    "url": "",
    "dockerfile": "docker/pyt_vllm",
    "scripts": "scripts/vllm/run.sh",
    "data": "huggingface",
    "n_gpus": "-1",
    "owner": "mad.support@amd.com",
    "training_precision": "",
    "multiple_results": "perf_Mistral-7B-v0.1-FP8-KV.csv",
    "tags": [
      "pyt",
      "vllm"
    ],
    "timeout": -1,
    "args":
    "--model_repo amd/Mistral-7B-v0.1-FP8-KV --test_option latency,throughput --num_gpu 1 --datatype float8 --tunableop off"
  },
  {
    "name": "pyt_vllm_dbrx_fp8",
    "url": "",
    "dockerfile": "docker/pyt_vllm",
    "scripts": "scripts/vllm/run.sh",
    "data": "huggingface",
    "n_gpus": "-1",
    "owner": "mad.support@amd.com",
    "training_precision": "",
    "multiple_results": "perf_dbrx-instruct-FP8-KV.csv",
    "tags": [
      "pyt",
      "vllm"
    ],
    "timeout": -1,
    "args":
    "--model_repo amd/dbrx-instruct-FP8-KV --test_option latency,throughput --num_gpu 8 --datatype float8 --tunableop off"
  },
  {
    "name": "pyt_vllm_command-r-plus_fp8",
    "url": "",
    "dockerfile": "docker/pyt_vllm",
    "scripts": "scripts/vllm/run.sh",
    "data": "huggingface",
    "n_gpus": "-1",
    "owner": "mad.support@amd.com",
    "training_precision": "",
    "multiple_results": "perf_c4ai-command-r-plus-FP8-KV.csv",
    "tags": [
      "pyt",
      "vllm"
    ],
    "timeout": -1,
    "args":
    "--model_repo amd/c4ai-command-r-plus-FP8-KV --test_option latency,throughput --num_gpu 8 --datatype float8 --tunableop off"
  },
  {
    "name": "dummy_multi",
    "dockerfile": "docker/dummy",
    "scripts": "scripts/dummy/run_multi.sh",
    "n_gpus": "1",
    "owner": "mad.support@amd.com",
    "training_precision": "",
    "tags": [
      "dummies"
    ],
    "args": "",
    "multiple_results": "perf_dummy.csv"
  }
]
